# ğŸš€ Airflow Castor - ETL Project with Apache Airflow

[![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-017CEE?style=for-the-badge&logo=Apache%20Airflow&logoColor=white)](https://airflow.apache.org/)
[![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-316192?style=for-the-badge&logo=postgresql&logoColor=white)](https://postgresql.org/)
[![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)](https://docker.com/)
[![PySpark](https://img.shields.io/badge/PySpark-E25A1C?style=for-the-badge&logo=apache-spark&logoColor=white)](https://spark.apache.org/)

## ğŸ“‹ Description

This project implements an ETL (Extract, Transform, Load) pipeline using Apache Airflow, PySpark, MinIO, and PostgreSQL. The system processes orders and products data, performing transformations with PySpark and storing the results in a PostgreSQL database.

## ğŸ—ï¸ Architecture

The project consists of the following components:

- ğŸ”„ **Apache Airflow**: Workflow orchestrator
- âš¡ **PySpark**: Data processing engine for ETL
- ğŸ—„ï¸ **MinIO**: S3-compatible object storage
- ğŸ˜ **PostgreSQL**: Relational database for storing processed data
- â˜ï¸ **BigQuery**: Customer data source (Google Cloud)

## âœ¨ Key Features

- ğŸ”„ **Complete ETL Pipeline**: Data extraction, transformation, and loading
- âš¡ **PySpark Processing**: Distributed and scalable transformations
- ğŸ’¾ **Hybrid Storage**: MinIO for intermediate data, PostgreSQL for final data
- â˜ï¸ **BigQuery Integration**: Customer data extraction from Google Cloud
- ğŸ›¡ï¸ **Error Handling**: Success and failure tasks with trigger rules
- ğŸ“¦ **Data Compression**: Optimized storage with Parquet format
- ğŸ”§ **Docker Support**: Containerized deployment
- ğŸ“Š **Real-time Monitoring**: Comprehensive logging and monitoring

## ğŸ“ Project Structure

```
airflow-castor/
â”œâ”€â”€ ğŸ“‚ dags/
â”‚   â”œâ”€â”€ ğŸ orders_dag.py          # Main orders processing DAG
â”‚   â””â”€â”€ ğŸ” credentials.py         # Credentials configuration
â”œâ”€â”€ ğŸ“‚ queries/
â”‚   â”œâ”€â”€ ğŸ—ƒï¸ customers_bigquery.sql # BigQuery SQL query
â”‚   â”œâ”€â”€ ğŸ“Š kpis.sql              # KPI queries
â”‚   â”œâ”€â”€ ğŸ“„ orders.json           # Sample orders data
â”‚   â”œâ”€â”€ ğŸ—„ï¸ postgres_tables.sql   # Table creation scripts
â”‚   â””â”€â”€ ğŸ“‹ products.csv          # Products data
â”œâ”€â”€ ğŸ“‚ include/
â”‚   â””â”€â”€ ğŸ”‘ bigquery-course-464012-acf411040090.json # GCP credentials
â”œâ”€â”€ ğŸ³ docker-compose.yml        # PostgreSQL configuration
â”œâ”€â”€ ğŸ³ docker-compose.override.yml # MinIO configuration
â”œâ”€â”€ ğŸ³ Dockerfile               # Custom image with Java and AWS JARs
â”œâ”€â”€ ğŸ“¦ requirements.txt         # Python dependencies
â”œâ”€â”€ ğŸ“¦ packages.txt            # System packages
â””â”€â”€ ğŸ“„ README.md               # Project documentation
```

## ğŸ› ï¸ Technologies Used

### ğŸ”§ Core
- ğŸ”„ **Apache Airflow 2.x**: Workflow orchestration
- ğŸ **Python 3.x**: Main programming language
- âš¡ **PySpark 3.5.4**: Distributed data processing

### ğŸ’¾ Storage
- ğŸ—„ï¸ **MinIO**: S3-compatible object storage
- ğŸ˜ **PostgreSQL 15**: Relational database
- â˜ï¸ **Google BigQuery**: Cloud data warehouse

### ğŸ“Š Data Processing
- ğŸ¼ **Pandas**: Data manipulation
- ğŸ¹ **PyArrow**: Parquet file processing
- ğŸ”— **Boto3**: AWS client for MinIO
- ğŸ“¦ **SQLAlchemy**: Database ORM

## ğŸ“Š Data Flow

1. **ğŸ“¥ Extraction**: 
   - ğŸ“‹ Product data from MinIO (CSV)
   - ğŸ“„ Orders data from MinIO (JSON)
   - ğŸ‘¥ Customer data from BigQuery

2. **ğŸ”„ Transformation**:
   - âš¡ Processing with PySpark
   - ğŸ§¹ Data cleaning and normalization
   - ğŸ“… Date and field formatting
   - ğŸ“¦ Snappy compression

3. **ğŸ“¤ Loading**:
   - ğŸ’¾ Temporary storage in MinIO (Parquet)
   - ğŸ˜ Final load to PostgreSQL
   - ğŸ—„ï¸ Table creation: `products`, `orders`, `customers`

## ğŸš€ Installation and Configuration

### ğŸ“‹ Prerequisites

- ğŸ³ Docker and Docker Compose
- ğŸ Python 3.8+
- â˜ï¸ Google Cloud Platform access (for BigQuery)
- â˜• Java 17+ (for PySpark)

### 1ï¸âƒ£ Clone the Repository

```bash
git clone <repository-url>
cd airflow-castor
```

### 2ï¸âƒ£ Configure Credentials

Edit the `dags/credentials.py` file with your credentials:

```python
# MinIO configuration
MINIO_ENDPOINT = "http://host.docker.internal:9000"
MINIO_ACCESS_KEY = "minio"
MINIO_SECRET_KEY = "minio12345"
MINIO_BUCKET_NAME = "orders"

# PostgreSQL configuration
POSTGRES_HOST = "airflow-castor_0f1e85-postgres-1"
POSTGRES_PORT = "5432"
POSTGRES_USER = "postgres"
POSTGRES_PASSWORD = "postgres"
POSTGRES_DB = "postgres"

# BigQuery configuration
GCP_PROJECT_ID = "your-gcp-project"
BQ_DATASET_TABLE = "airflow.customers"
```

### 3ï¸âƒ£ Configure Docker Network

```bash
docker network create airflow
```

### 4ï¸âƒ£ Start Services

```bash
# Start PostgreSQL
docker-compose up -d

# Start MinIO
docker-compose -f docker-compose.override.yml up -d
```

### 5ï¸âƒ£ Configure MinIO

Access the MinIO console at `http://localhost:9001` and create the `orders` bucket.

### 6ï¸âƒ£ Load Sample Data

Upload files from `queries/` to the MinIO bucket:
- ğŸ“‹ `products.csv` â†’ `raw_data/products/`
- ğŸ“„ `orders.json` â†’ `raw_data/orders/`

## âš™ï¸ Airflow Configuration

### ğŸ”§ Environment Variables

Configure the following variables in Airflow:

- ğŸ˜ `postgres_default`: PostgreSQL connection
- â˜ï¸ `bigquery_default`: BigQuery connection
- ğŸ—„ï¸ `aws_default`: MinIO connection (using MinIO credentials)

### ğŸ”— Required Connections

1. ğŸ˜ **PostgreSQL**: `postgres_default`
2. â˜ï¸ **BigQuery**: `bigquery_default`
3. ğŸ—„ï¸ **AWS/MinIO**: `aws_default`

### ğŸ” Connection Details

#### PostgreSQL Connection
- **Connection ID**: `postgres_default`
- **Connection Type**: `Postgres`
- **Host**: `airflow-castor_0f1e85-postgres-1`
- **Schema**: `postgres`
- **Login**: `postgres`
- **Password**: `postgres`
- **Port**: `5432`

#### BigQuery Connection
- **Connection ID**: `bigquery_default`
- **Connection Type**: `Google Cloud`
- **Project ID**: Your GCP project ID
- **Keyfile JSON**: Path to your service account JSON

#### MinIO Connection
- **Connection ID**: `aws_default`
- **Connection Type**: `Amazon Web Services`
- **Login**: `minio`
- **Password**: `minio12345`
- **Extra**: `{"endpoint_url": "http://host.docker.internal:9000"}`

## ğŸ“ˆ Monitoring and Logs

- ğŸŒ **Airflow UI**: `http://localhost:8080`
- ğŸ—„ï¸ **MinIO Console**: `http://localhost:9001`
- ğŸ“‹ **Logs**: Available in the Airflow interface
- ğŸ“Š **DAG Status**: Monitor task execution in real-time

## ğŸ”„ DAG Tasks

### `orders_dag`

1. ğŸš€ **start**: Initial task
2. ğŸ‘¥ **insert_data_to_postgres**: Customer data extraction from BigQuery
3. âš¡ **run_pyspark_etl**: ETL processing with PySpark
4. ğŸ“¤ **load_parquet_to_postgres**: Loading transformed data to PostgreSQL
5. âœ… **success_task**: Success handling
6. âŒ **failed_task**: Error handling

### ğŸ“‹ Task Dependencies

```
start â†’ insert_data_to_postgres â†’ run_pyspark_etl â†’ load_parquet_to_postgres
                                                      â†“
                                              [success_task, failed_task]
```

## ğŸ—„ï¸ Data Structure

### ğŸ“¦ `products` Table
- ğŸ“‹ Product fields with metadata
- â° Load timestamp
- ğŸ†” Primary key: `product_id`

### ğŸ“„ `orders` Table
- ğŸ”„ Transformed order information
- ğŸ§® Calculated fields: `cash_or_card`, `is_delivered`
- ğŸ“… Normalized dates
- ğŸ†” Primary key: `id`

### ğŸ‘¥ `customers` Table
- ğŸ‘¤ Customer data from BigQuery
- ğŸ“Š Demographic and contact information
- ğŸ†” Primary key: `id`

## ğŸ› Troubleshooting

### âš ï¸ Common Issues

1. **ğŸ—„ï¸ MinIO connection error**:
   - âœ… Verify that MinIO is running
   - ğŸ” Check credentials in `credentials.py`
   - ğŸŒ Ensure network connectivity

2. **âš¡ PySpark error**:
   - â˜• Verify that Java 17+ is installed
   - ğŸ“¦ Check AWS JARs in `/opt/spark/jars/`
   - ğŸ’¾ Ensure sufficient memory allocation

3. **ğŸ˜ PostgreSQL error**:
   - âœ… Verify that the database is running
   - ğŸŒ Check Docker network configuration
   - ğŸ” Verify connection credentials

4. **â˜ï¸ BigQuery error**:
   - ğŸ”‘ Check GCP service account permissions
   - ğŸ“„ Verify JSON credentials file
   - ğŸŒ Ensure network access to Google Cloud

### ğŸ“‹ Useful Logs

```bash
# Airflow logs
docker logs <airflow-container>

# MinIO logs
docker logs airflow_minio1

# PostgreSQL logs
docker logs airflow_postgres

# Check container status
docker ps -a
```

### ğŸ”§ Performance Optimization

- âš¡ **Spark Configuration**: Adjust memory settings in PySpark
- ğŸ’¾ **Database Tuning**: Optimize PostgreSQL settings
- ğŸ“¦ **Compression**: Use appropriate compression for Parquet files
- ğŸ”„ **Parallel Processing**: Configure task parallelism

## ğŸ§ª Testing

### ğŸ§ª Unit Tests

```bash
# Run DAG tests
python -m pytest tests/dags/

# Test individual components
python -m pytest tests/dags/test_orders_dag.py
```

### ğŸ” Integration Tests

```bash
# Test database connections
python tests/test_connections.py

# Test data pipeline
python tests/test_etl_pipeline.py
```

## ğŸ“Š Performance Metrics

- â±ï¸ **Processing Time**: Monitor ETL execution time
- ğŸ“ˆ **Data Volume**: Track data processing capacity
- ğŸ”„ **Success Rate**: Monitor task success/failure rates
- ğŸ’¾ **Resource Usage**: CPU, memory, and storage utilization

## ğŸ”’ Security Considerations

- ğŸ” **Credentials**: Store sensitive data in environment variables
- ğŸŒ **Network Security**: Use secure connections for production
- ğŸ”‘ **Access Control**: Implement proper authentication
- ğŸ“‹ **Audit Logging**: Enable comprehensive logging

## ğŸ“„ License

This project is under the MIT License. See the `LICENSE` file for more details.

## ğŸ‘¥ Authors

- **Diego Castellanos** - *Initial Development* - [GitHub](https://github.com/diego)

## ğŸ¤ Contributing

1. ğŸ´ Fork the project
2. ğŸŒ¿ Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. ğŸ’¾ Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. ğŸ“¤ Push to the branch (`git push origin feature/AmazingFeature`)
5. ğŸ”€ Open a Pull Request

## ğŸ™ Acknowledgments

- ğŸ”„ Apache Airflow Community
- âš¡ PySpark Documentation
- ğŸ—„ï¸ MinIO Documentation
- ğŸ˜ PostgreSQL Community
- â˜ï¸ Google Cloud Platform

## ğŸ“š Additional Resources

- ğŸ“– [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- âš¡ [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
- ğŸ—„ï¸ [MinIO Documentation](https://docs.min.io/)
- ğŸ˜ [PostgreSQL Documentation](https://www.postgresql.org/docs/)

---

**âš ï¸ Note**: This project is an educational example of ETL implementation with Apache Airflow. Make sure to configure the appropriate credentials before using in production.

**ğŸš€ Happy Data Processing!** ğŸ‰